\documentclass[12pt]{article}
\input{preamble}
\usepackage{geometry}
% \geometry{margin=1in}

\title{UMC202: Review Notes}
\author{Naman Mishra}
\date{18 October, 2023}

\begin{document}
\maketitle

\section{Root-finding Methods} \label{sec:root_finding}
\begin{definition}[Rate of convergence] \label{def:root_finding:rate}
    Let $\set{\alpha}_{n=1}^\infty$ and $\set{\beta}_{n=1}^\infty$ be sequences
    that converge to $\alpha$ and $0$ respectively.
    If $k \in \R^+$ and $n \in \N^+$ exists such that \[
        \abs{\alpha_{n+1} - \alpha} \leq k \abs{\beta_n} \quad \text{for all $n \geq N$,}
    \] then $\set{\alpha_{n+1}}$ is said to converge to $\alpha$ with rate of
    convergence $O(\beta_n)$ and written as \[
        \alpha_n = \alpha + O(\beta_n).
    \]
\end{definition}
\begin{definition}[Order of convergence] \label{def:root_finding:order}
    Let $\set{p_n}_{n=0}^\infty$ converge to $p$ but never equal it.
    If positive constants $\alpha$ and $\lambda$ exist with \[
        \lim_{n \to \infty} \frac{\abs{p_{n+1} - p}}{\abs{p_n - p}^\alpha}
            = \lambda,
    \] then $\set{p_n}$ is said to converge to $p$ \emph{of order} $\alpha$ with
    \emph{asymptotic error constant} $\lambda$.
\end{definition}
\begin{remark}
    If $\alpha = 1$, $\set{p_n}$ is said to converge \emph{linearly} to $p$. \\
    If $\alpha = 2$, $\set{p_n}$ is said to converge \emph{quadratically} to
        $p$.
\end{remark}
\subsection{Bisection Method} \label{sec:bisection}
An algorithm is described in \cref{alg:bisection}.
\begin{theorem}[Convergence] \label{thm:bisection:convergence}
    Suppose that $f \in C^0[a, b]$ and $f(a)f(b) < 0$.
    Then the bisection method generates a sequence $\set{p_n}_{n=1}^\infty$
    that converges to a root $p$ of $f$ with \[
        \abs{p_n - p} \leq \frac{b - a}{2^n} \quad \text{for all $n$.}
    \]
\end{theorem}
\begin{proof} Induction. \end{proof}

\subsection{Fixed Point Iteration} \label{sec:fixed_point}
\vskip 1em
\begin{definition}[Fixed point] \label{def:fixed_point}
    A number $p$ is said to be a fixed point of a function $g$ if $g(p) = p$.
\end{definition}
\begin{theorem} \label{thm:fixed_point:existence}
    Let $g : [a, b] \to [a, b]$ be continuous.
    Then $g$ has a fixed point in $[a, b]$.

    Moreover, if $g$ is differentiable on $(a, b)$ with $g'(x) < 1$ for all
    $x \in (a, b)$, then the fixed point is unique.
\end{theorem}
\begin{proof}
    Let $g : [a, b] \to [a, b]$ be continuous.
    If $g(a) = a$ or $g(b) = b$, we are done.
    Otherwise, $g(a) - a > 0$ and $g(b) - b < 0$.
    By IVT, there exists $p \in (a, b)$ such that $g(p) = p$.

    Now suppose that $g'$ exists on $(a, b)$ with $g' < 1$.
    Let $x, y \in (a, b)$ be fixed points of $g$.
    If they are distinct, then by MVT, there exists $c \in (x, y)$ such that \[
        g'(c) = \frac{g(y) - g(x)}{y - x} = \frac{y - x}{y - x} = 1,
    \] which is a contradiction.
\end{proof}
The fixed point iteration method is described in \cref{alg:fixed_point}.
\begin{theorem}[Fixed point theorem] \label{thm:fixed_point:convergence}
    Let $g \in C^0[a, b]$ be such that $g(x) \in [a, b]$ for all $x \in [a, b]$.
    Suppose further that $g$ is differentiable on $(a, b)$ and that a constant
    $0 < k < 1$ exists with $g'(x) \leq k < 1$ for all $x \in (a, b)$.
    
    Then for any $p_0 \in [a, b]$, the sequence $\set{p_n}_{n=1}^\infty$
    generated by $p_n = g(p_{n-1})$ converges to the unique fixed point $p$ in
    $[a, b]$ with rate of convergence $O(k^n)$.
\end{theorem}
\begin{proof}
    Let $p$ be the unique fixed point of $g$ in $[a, b]$, whose existence is
    guaranteed by \cref{thm:fixed_point:existence}.

    Let $p_0 \in [a, b]$.
    Then for any $n \in \N^+$, there exists by MVT a $c_n \in (a, b)$ such that
    \[
        \frac{\abs{g(p_n) - g(p)}}{\abs{p_n - p}}
            = \frac{\abs{p_{n+1} - p}}{\abs{p_n - p}}
            = \abs{g'(c_n)} \leq k.
    \] Thus by induction, \[
        \abs{p_n - p} \leq k^n \abs{p_0 - p}
                 \leq k^n \max\set{p_0 - a, b - p_0}. \qedhere
    \]
\end{proof}
\begin{corollary} \label{thm:fixed_point:bound}
    If $g$ satisfies the hypotheses of \cref{thm:fixed_point:convergence},
    then for any $p_0 \in [a, b]$, \begin{align*}
        \abs{p_n - p} &\leq k^n \max\set{p_0 - a, b - p_0}, \text{ and} \\
        \abs{p_n - p} &\leq \frac{k^n}{1 - k} \abs{p_1 - p_0}.
    \end{align*}
\end{corollary}
\begin{proof}
    The first inequality is proved in the proof of
    \cref{thm:fixed_point:convergence}.

    For the second inequality, note that \begin{align*}
        \abs{p_0 - p} &= \abs{p_0 - p_1 + p_1 - p} \\
                 &\leq \abs{p_0 - p_1} + \abs{p_1 - p} \\
                 &\leq \abs{p_0 - p_1} + k \abs{p_0 - p} \\
        \implies \abs{p_0 - p} &\leq \frac{1}{1 - k} \abs{p_1 - p_0}. \\
        \shortintertext{Thus using}
        \abs{p_n - p} &\leq k^n \abs{p_0 - p} \\
        \shortintertext{we get}
        \abs{p_n - p} &\leq \frac{k^n}{1 - k} \abs{p_1 - p_0}. \qedhere
    \end{align*}
\end{proof}
\subsection{Newton-Raphson Method} \label{sec:newton}
Fixed point iteration with $g(x) = x - \frac{f(x)}{f'(x)}$.
An algorithm is described in \cref{alg:newton}.
\begin{theorem}[Convergence] \label{thm:newton:convergence}
    Let $f \in C^2[a, b]$ have a root $p$ in $[a, b]$ such that $f'(p) \neq 0$.
    Then there exists a $\delta > 0$ such that for any $p_0 \in N_\delta(p)$,
    the sequence $\set{p_n}_{n=1}^\infty$ generated by Newton's method converges
    to $p$.
\end{theorem}
\begin{proof}
    Let $g(x) = x - \frac{f(x)}{f'(x)}$.
    Newton's method is described by $p_n = g(p_{n-1})$ for $n \geq 1$.

    Since $f'$ is continuous on $[a, b]$, there exists a $\delta_1 > 0$ such
    $f'(x) \neq 0$ for all $x \in N_{\delta_1}(p)$.
    Thus $g$ is defined on $N_{\delta_1}(p)$.

    $g'(x) = \frac{f(x)f''(x)}{f'(x)^2}$.
    Since $f \in C^2[a, b]$, $g'$ is continuous on $N_{\delta_1}(p)$.
    
    Let $0 < k < 1$.
    Since $g'(p) = 0$ and $g'$ is continuous, there exists a neighborhood
    $N_\delta(p)$ on which $\abs{g'}$ is bounded by $k$.
    By MVT, this implies that $g$ maps $N_\delta(p)$ into itself.

    Thus by \cref{thm:fixed_point:convergence}, the sequence $\set{p_n}$
    converges to the unique fixed point $p$ of $g$ for all initial
    approximations $p_0 \in N_\delta(p)$.
\end{proof}

\begin{theorem}
    Let $g$ follow all hypotheses of \cref{thm:fixed_point:convergence}.
    % That is, $g \in C^0[a, b]$ such that $g(x) \in [a, b]$ for all
    % $x \in [a, b]$, $g$ is differentiable on $(a, b)$, and a constant
    % $0 < k < 1$ exists with $g'(x) \leq k < 1$ for all $x \in (a, b)$.

    If $g'(p) \neq 0$, then for any
    $p_0 \in N_\delta(p) \setminus \set{p}$, the sequence $p_n = g(p_{n-1})$
    converges only linearly to $p$.
\end{theorem}
\begin{proof}
    By MVT,
    \begin{align*}
        \lim_{n \to \infty} \frac{\abs{p_{n+1} - p}}{\abs{p_n - p}}
            &= \lim_{n \to \infty} \abs{g'(\xi_n)} \\
            &= \abs{g'(\lim_{n \to \infty} \xi_n)} \\
            &= \abs{g'(p)}
    \end{align*}
    where $\xi_n$ is between $p_n$ and $p$.
\end{proof}

\begin{theorem} \label{thm:newton:quadratic}
    Let $g \in C^2(I)$ for some open interval $I$ containing a fixed point $p$
    of $g$.
    Suppose that $g'(p) = 0$.

    Then there exists a $\delta > 0$ such that for any $p_0 \in N_\delta(p)$,
    the sequence $p_n = g(p_{n-1})$ converges at least quadratically to $p$.

    Moreover, if $g''$ is strictly bounded by $M$ on $I$, then \[
        \abs{p_{n+1} - p} < \frac{M}{2} \abs{p_n - p}^2
    \] \textcolor{red!70}{for large $n$}. (Or even for any $n$?)
\end{theorem}
\begin{proof}
    Let $0 < k < 1$.
    Choose $\delta_0$ such that $\abs{g'} \leq k$ on $N_{\delta_0}(p)$.
    Let $p_0$ be in $N_{\delta_0}(p)$.
    Then we know from \cref{thm:fixed_point:convergence} that 
    $g$ maps $N_{\delta_0}(p)$ into itself and the sequence $p_n = g(p_{n-1})$
    converges to $p$.

    By Taylor's theorem,
    \begin{align*}
        g(p_n) - g(p) &= g'(p) (p_n - p) + \frac{g''(\xi_n)}{2} (p_n - p)^2\\
        p_{n+1}  -   p  &= \frac{g''(\xi_n)}{2} (p_n - p)^2. \tag{$*$}
            \label{eq:newton:quadratic:bound}
    \end{align*}
    where $\xi_n$ is between $p_n$ and $p$.
    Thus $\lim_{n \to \infty} \xi_n = p$ and since $g''$ is continuous, \[
        \lim_{n \to \infty} \frac{p_{n+1} - p}{(p_n - p)^2} = \frac{g''(p)}{2}
    \] and so $p_n$ converges at least quadratically.

    If $g''$ is strictly bounded by $M$ on $I$, then 
    \cref{eq:newton:quadratic:bound} implies that \[
        \abs{p_{n+1} - p} < \frac{M}{2} \abs{p_n - p}^2. \qedhere
    \]
\end{proof}
\begin{corollary} \label{thm:newton:rate}
    Newton's method exhibits quadratic convergence.
\end{corollary}

\subsection{Secant Method} \label{sec:secant}
The secant method is described in \cref{alg:secant}.
\subsection{Regula Falsi Method} \label{sec:regula_falsi}
In the bisection method, the root is always bracketed in the interval
$[a_n, b_n]$.
In the Newton-Raphson and secant methods, no such guarantee is afforded.
Regula falsi is a modification of the secant method that guarantees root
bracketing.
It is described in \cref{alg:regula_falsi}.

\section{Interpolation} \label{sec:interpolation}
\begin{definition}[Lagrange basis] \label{def:interpolation:lagrange}
    Let $S = \set{x_0, \dots, x_n}$ be a set of $n + 1$ distinct points in $\R$.
    Then the \emph{Lagrange basis} $\set{L_0^S, \dots, L_n^S}$ is defined as \[
        L_i^S(x) = \prod_{\substack{j = 0 \\ j \neq i}}^n
            \frac{x - x_j}{x_i - x_j}
    \] for $i \in \set{0, \dots, n}$.
\end{definition}
\begin{theorem}[Unique interpolation] \label{thm:interpolation:unique}
    Let $f : D \subseteq \R \to \R$ be a function.
    Let $S = \set{x_0, \dots, x_n}$ be distinct points in $D$.
    Then there exists a unique polynomial $p$ of degree at most $n$ such that
    $p(x_i) = f(x_i)$ for all $i \in \set{0, \dots, n}$, given by \[
        p(x) = \sum_{i=0}^n f(x_i) L_i^S(x).
    \]
\end{theorem}
\begin{proof}
    \textbf{Existence:} Note that
    $L_i^S(x_i) = \prod_{j \neq i} \frac{x_i - x_j}{x_i - x_j} = 1$ and for
    $i \neq k$, \begin{align*}
        L_i^S(x_k) &= \prod_{\substack{j = 0 \\ j \neq i}}^n
                \frac{x_k - x_j}{x_i - x_j} \\
            &= \prod_{\substack{j = 0 \\ j \neq i, k}}^n
                \frac{x_k - x_j}{x_i - x_j} \cdot \frac{x_k - x_k}{x_i - x_k} \\
            &= 0.
    \end{align*}
    Thus \begin{align*}
        p(x_k) &= \sum_{i=0}^n f(x_i) L_i^S(x_k) \\
              &= \sum_{i=0}^n f(x_i) \delta_{ik} \\
              &= f(x_k)
    \end{align*}
    where $\delta$ is the Kronecker delta.

    \textbf{Uniqueness:} Suppose that $p$ and $q$ are polynomials of degree at
    most $n$ that agree with $f$ at $x_0, \dots, x_n$.
    Then $r = p - q$ is a polynomial of degree at most $n$ that has at least
    $n + 1$ roots.
    By the fundamental theorem of algebra, $r$ is the zero polynomial.
\end{proof}
\begin{corollary}
    Lagrange basis polynomials form a basis for $\mathcal{P}_n$, the space of
    polynomials of degree at most $n$.
\end{corollary}
\begin{proof}
    Let $S = \set{x_0, \dots, x_n}$ be distinct points in $\R$.
    Let $p$ be a polynomial of degree at most $n$.
    Then $p = \sum_{i=0}^n p(i) L_i^S$.

    Thus the Lagrange basis polynomials span the space.
    Since $\dimn \mathcal{P}_n = n + 1$, they form a basis.
\end{proof}
\begin{definition}[Newton basis] \label{def:interpolation:newton_basis}
    Let $S = \set{x_0, \dots, x_n}$ be a set of $n + 1$ distinct points in $\R$.
    Then the \emph{Newton basis} $\set{N_0^S, \dots, N_n^S}$ is defined as \[
        N_i^S(x) = \prod_{j = 0}^{i-1} (x - x_j)
    \] for $i \in \set{0, \dots, n}$.
\end{definition}
\begin{theorem} \label{thm:interpolation:newton_basis}
    The Newton basis $\set{N_0^S, \dots, N_n^S}$ is a basis for $\mathcal{P}_n$.
\end{theorem}

\section{Initial Value Problem} \label{sec:ivp}
\subsection{Central differences} \label{sec:central_differences}
From Taylor's theorem,
\begin{align*}
    y_{i+1} &= y_i + h y'(t_i) + \frac{h^2}{2} y''(t_i)
            + \frac{h^3}{3!} y'''(\xi_i) \\
    y_{i-1} &= y_i - h y'(t_i) + \frac{h^2}{2} y''(t_i)
            - \frac{h^3}{3!} y'''(\eta_i) \\
    % y_{i+1} - 2y_i + y_{i-1} &= h^2 y''(t_i) + O(h^3)
    y_{i+1} - y_{i-1} &= 2h y'(t_i) +
            \frac{h^3}{3!} (y'''(\xi_i) + y'''(\eta_i))
\end{align*}
Dropping the remainder term gives the central differences scheme.
\begin{align*}
    w_0 &= \alpha \\
    w_1 &= \alpha_1 \\
    w_{i+1} &= w_{i-1} + 2h f(t_i, w_i)
\end{align*}
% Applying this for \[
%     y' = -2y \quad y(0) = 1
% \] gives \[
%     w_{i+1} + 4h w_i - w_{i-1} = 0.
% \] We check if a solution of the form $w_i = \lambda^i$ exists.
% \begin{align*}
%     \lambda^{i+1} + 4h \lambda^i - \lambda^{i-1} &= 0 \\
%     \lambda^2   + 4h \lambda  - 1    &= 0
% \end{align*}
% Thus \begin{align*}
%     \lambda &= \frac{-4h \pm \sqrt{16h^2 + 4}}{2} \\
%       &= -2h \pm \sqrt{1 + 4h^2} \\
%       &= \pm 1 - 2h + O(h^2)
% \end{align*}
% The general solution is of the form \begin{align*}
%     w_i &= A \left(1 - 2h + O(h^2)\right)^i
%             + B \left(-1 - 2h + O(h^2)\right)^i \\
%        &= A \left(1 - 2\frac{t_i}{i} + O\left(\frac{1}{i^2}\right)\right)^i
%             + B \left(-1 - 2\frac{t_i}{i} + O\left(\frac{1}{i^2}\right)\right)^i
% \end{align*} As $h \to 0$, \[
%     w(t_i) \to A e^{-2t_i} + B (-1)^i e^{2t_i}.
% \] The correct solution is $y(t) = e^{-2t}$.
% The second term in the above expression causes instability.
%
% However, if the IVP was \[
%     y' = 2y \quad \text{with} \quad y(0) = 1,
% \] the scheme would be stable.
\subsection{Stability} \label{sec:stability}
\subsubsection{Multistep methods} \label{sec:multistep}
\begin{definition}[Characteristic polynomial] \label{def:multistep:character}
    For the multistep method
    $w_0 = \alpha, w_1 = \alpha_1, \dots, w_n = \alpha_n$ and \[
        w_{i+1} = a_{m-1} w_i + a_{m-2} w_{i-1} + \dots + a_0 w_{i+1-m}
            + h F(t_i, h, w_{i+1}, w_i, \dots, w_{i+1-m}),
    \] we define the characteristic polynomial $P$ as \[
        P(\lambda) = \lambda^m - a_{m-1} \lambda^{m-1} - \dots - a_0.
    \]
\end{definition}
The stability of the multistep method with respect to round off errors is
dictated by the magnitudes of the roots of $P$.
\begin{definition}[Root condition] \label{def:stability:root_condition}
    Let $\lambda_1, \dots, \lambda_n$ be the $n$ roots of the characteristic
    polynomial equation \[
        P(\lambda) = \lambda^m - a_{m-1} \lambda^{m-1} - \dots - a_0 = 0.
    \] associated with the multistep method described above.

    If $\abs{\lambda_i} \leq 1$ and all roots with magnitude $1$ are simple,
    then the difference method is said to satisfy the \emph{root condition}.
\end{definition}
\hfill \begin{minipage}{0.9\textwidth}
    \begin{definition} \label{def:multistep:stable} \leavevmode
        \begin{itemize}
            \item A multistep method that satisfies the root condition and has
                $1$ as the only root of magnitude $1$ is said to be
                \emph{strongly stable}.
            \item A multistep method that satisfies the root condition and has
                more than one distinct root of magnitude $1$ is said to be
                \emph{weakly stable}.
            \item A multistep method that does not satisfy the root condition
                is said to be \emph{unstable}.
        \end{itemize}
    \end{definition}
\end{minipage}

\section{Algorithms} \label{sec:algorithms}
\begin{algorithm}
    \caption{Bisection method of finding a root}
    \label{alg:bisection}
    \begin{algorithmic}[1]
    \Require{A continuous function $f$ on $[a, b]$ such that $f(a)f(b) < 0$,
        a tolerance $\epsilon > 0$,
        and a maximum number of iterations $N$}
    \Ensure{An approximate root of $f$ on $[a, b]$, or \Nil\ if not found}
        \For{$i \gets 1$ to $N$}
            \State $p \gets \frac{a + b}{2}$
            \State $f_p \gets f(p)$
            \If{$f_p = 0$ or $\frac{b - a}{2} < \epsilon$}
                \State \Return $p$
            \EndIf
            \If{$f(a) \cdot f_p < 0$}
                \State $b \gets p$
            \Else
                \State $a \gets p$
            \EndIf
        \EndFor
        \State \Return \Nil %\Comment{Maximum number of iterations exceeded}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Fixed point iteration method of finding a root}
    \label{alg:fixed_point}
    \begin{algorithmic}[1]
        \Require{A function $g : [a, b] \to [a, b]$ that follows
            the conditions of \cref{thm:fixed_point:convergence},
            an initial guess $p_0 \in [a, b]$,
            a tolerance $\epsilon > 0$,
            and a maximum number of iterations $N$}
        \Ensure{An approximate fixed point of $g$ on $[a, b]$,
            or \Nil\ if not found}
        \For{$i \gets 1$ to $N$}
            \State $p \gets g(p_0)$
            \If{$\abs{p - p_0} < \epsilon$}
                \State \Return $p$
            \EndIf
            \State $p_0 \gets p$
        \EndFor
        \State \Return \Nil %\Comment{Maximum number of iterations exceeded}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Newton-Raphson method for finding a root}
    \label{alg:newton}
    \begin{algorithmic}[1]
        \Require{A function $f$ that follows the hypotheses of
            \cref{thm:newton:convergence}, an initial guess $p_0$,
            a tolerance $\epsilon > 0$,
            and a maximum number of iterations $N$}
        \Ensure{An approximate root of $f$, or \Nil\ if not found}
        \For{$i \gets 1$ to $N$}
            \State $p \gets p_0 - \frac{f(p_0)}{f'(p_0)}$
            \If{$\abs{p - p_0} < \epsilon$}
                \State \Return $p$
            \EndIf
            \State $p_0 \gets p$
        \EndFor
        \State \Return \Nil %\Comment{Maximum number of iterations exceeded}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Secant method for finding a root}
    \label{alg:secant}
    \begin{algorithmic}[1]
        \Require{A function $f$ that follows God knows what conditions,
            initial guesses $p_0$ and $p_1$,
            a tolerance $\epsilon > 0$,
            and a maximum number of iterations $N$}
        \Ensure{An approximate root of $f$, or \Nil\ if not found}
        \State $(q_0, q_1) \gets (f(p_0), f(p_1))$
        \For{$i \gets 2$ to $N$}
            \State $p \gets \frac{p_0 q_1 - p_1 q_0}{q_1 - q_0}$
                \Comment{External $q_0:q_1$? Or internal $\abs{q_0}:\abs{q_1}$?}
            \State $(p_0, q_0) \gets (p_1, q_1)$
            \State $(p_1, q_1) \gets (p, f(p))$
            \If{$q_1 = 0$ or $\abs{p_1 - p_0} < \epsilon$}
                \State \Return $p_1$
            \EndIf
        \EndFor
        \State \Return \Nil %\Comment{Maximum number of iterations exceeded}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{Regula falsi method for finding a root}
    \label{alg:regula_falsi}
    \begin{algorithmic}[1]
        \Require{A function $f$ that follows God knows what conditions,
            initial guesses $p_0$ and $p_1$,
            a tolerance $\epsilon > 0$,
            and a maximum number of iterations $N$}
        \Ensure{An approximate root of $f$, or \Nil\ if not found}
        \State $(q_0, q_1) \gets (f(p_0), f(p_1))$
        \For{$i \gets 2$ to $N$}
            \State $p \gets \frac{p_0 q_1 - p_1 q_0}{q_1 - q_0}$
            \State $q \gets f(p)$
            \If{$q = 0$ or $\abs{p - p_1} < \epsilon$}
                \State \Return $p$
            \EndIf
            \If{$q_0 \cdot q < 0$}
                \State $(p_1, q_1) \gets (p, q)$
            \Else
                \State $(p_0, q_0) \gets (p, q)$
            \EndIf
        \EndFor
        \State \Return \Nil
    \end{algorithmic}
\end{algorithm}
\end{document}
